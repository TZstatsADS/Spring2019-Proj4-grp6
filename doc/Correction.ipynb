{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Importing libraries for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Labelling tesseact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "Applied Data Science \n",
    "Project 4 - Detection\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#import feature as feature\n",
    "\n",
    "\n",
    "def labelTesseract():\n",
    "        \n",
    "\n",
    "    \n",
    "    truth_files_list = glob.glob('../data/ground_truth/*.txt') \n",
    "    test_files_list = glob.glob('../data/tesseract/*.txt')\n",
    "\n",
    "    # only taking the ones that have the same number of lines in the file\n",
    "\n",
    "    truth_files = []\n",
    "    test_files = []\n",
    "    file_counts = 0 # store number of files (test and truth have same length)\n",
    "\n",
    "    for truth, test in zip(truth_files_list, test_files_list):\n",
    "        truth_length = len(open(truth).readlines())    \n",
    "        test_length = len(open(test).readlines())\n",
    "        if truth_length == test_length:\n",
    "            file_counts += 1\n",
    "            truth_files.append(truth)\n",
    "            test_files.append(test)\n",
    "\n",
    "\n",
    "    # only taking lines that have the same number of words\n",
    "    truth_words = []\n",
    "    test_words = []\n",
    "    truth_test_pair = [] # for correction\n",
    "    actual_counts = 0 # actual counts of numbers of words after filtering\n",
    "    for truth, test in zip(truth_files, test_files):\n",
    "            \n",
    "        with open(truth) as fd_truth:\n",
    "            with open(test) as fd_test:\n",
    "                for truth_line, test_line in zip(fd_truth, fd_test):\n",
    "                    tmp_truth = truth_line.strip().split()\n",
    "                    tmp_test = test_line.strip().split()\n",
    "                    if len(tmp_truth) == len(tmp_test):\n",
    "                        for truth_word, test_word in zip(tmp_truth, tmp_test):\n",
    "                            actual_counts += 1\n",
    "                            truth_words.append(truth_word)\n",
    "                            test_words.append(test_word)\n",
    "                            truth_test_pair.append((truth_word, test_word))\n",
    "    # uncomment below for testing\n",
    "    \n",
    "    # print(actual_counts)\n",
    "    print(len(truth_words))\n",
    "    print(len(test_words))\n",
    "    print(truth_words[:20])\n",
    "    print(test_words[:20])\n",
    "    \n",
    "    '''\n",
    "    # from the lists of words (truth, test) compare each of them\n",
    "    # label 1 if test is the same as truth (correct)\n",
    "    # label 0 if test is the different (wrong)\n",
    "\n",
    "    label_dict = defaultdict(int)\n",
    "\n",
    "    for truth, test in zip(truth_words, test_words):\n",
    "        if truth == test:\n",
    "            label_dict[test] = 1\n",
    "        else:\n",
    "            label_dict[test] = 0\n",
    "\n",
    "    print(label_dict)\n",
    "    '''\n",
    "\n",
    "    # due to not being able to store duplicates, switching to list\n",
    "\n",
    "    label = []\n",
    "    for truth, test in zip(truth_words, test_words):\n",
    "        if truth == test:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    \n",
    "    # uncomment below for commenting\n",
    "    \n",
    "    print(label[:20])\n",
    "    \n",
    "\n",
    "    return (truth_test_pair, test_words, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Divide into test/train (by default 20%, 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_train(pair, label, k = 0.2):\n",
    "\n",
    "    # data = pd.DataFrame(words)\n",
    "    # split up data into k / 1-k percentage -- by defauly 80% train 20% test\n",
    "    train_data, test_data, train_label, test_label = train_test_split(pair, label, test_size = k)\n",
    "\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X_train_truth = []\n",
    "    X_test_truth = []\n",
    "    for data in train_data:\n",
    "        X_train.append(data[1])\n",
    "        X_train_truth.append(data[0])\n",
    "    for data in test_data:\n",
    "        X_test.append(data[1])\n",
    "        X_test_truth.append(data[0])\n",
    "\n",
    "\n",
    "\n",
    "    return (X_train, X_test, train_label, test_label, X_train_truth, X_test_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildFeatures(train_data, bigram_dict):\n",
    "    # f1\n",
    "    length = []\n",
    "    \n",
    "    # f2\n",
    "    v_count = []\n",
    "    c_count = []\n",
    "    v_div_l = []\n",
    "    c_div_l = []\n",
    "    v_div_c = []\n",
    "    \n",
    "    # f3\n",
    "    non_alnum = []\n",
    "    non_alnum_div_l = []\n",
    "    \n",
    "    # f4\n",
    "    digit = []\n",
    "    digit_l = []\n",
    "\n",
    "    # f5\n",
    "    lower = []\n",
    "    upper = []\n",
    "    lower_div_l = []\n",
    "    upper_div_l = []\n",
    "\n",
    "    #f6\n",
    "    three_consec_cons = []\n",
    "\n",
    "    #f7\n",
    "    alpha_num = []\n",
    "\n",
    "    #f8\n",
    "    six_consec_cons = []\n",
    "\n",
    "    #f9\n",
    "    infix = []\n",
    "\n",
    "    #f10\n",
    "    bigram = []\n",
    "\n",
    "    #f11\n",
    "    most_freq = []\n",
    "\n",
    "    #f12\n",
    "    non_div_alpha = []\n",
    "\n",
    "    for word in train_data:\n",
    "        length.append(f_1(word))\n",
    "        \n",
    "        v_count.append(f_2(word)[0])\n",
    "        c_count.append(f_2(word)[1])\n",
    "        v_div_l.append(f_2(word)[2])\n",
    "        c_div_l.append(f_2(word)[3])\n",
    "        v_div_c.append(f_2(word)[4])\n",
    "        \n",
    "        non_alnum.append(f_3(word)[0])\n",
    "        non_alnum_div_l.append(f_3(word)[1])\n",
    "\n",
    "        digit.append(f_4(word)[0])\n",
    "        digit_l.append(f_4(word)[1])\n",
    "\n",
    "        lower.append(f_5(word)[0])\n",
    "        upper.append(f_5(word)[1])\n",
    "        lower_div_l.append(f_5(word)[2])\n",
    "        upper_div_l.append(f_5(word)[3])\n",
    "\n",
    "        three_consec_cons.append(f_6(word))\n",
    "\n",
    "        alpha_num.append(f_7(word))\n",
    "\n",
    "        six_consec_cons.append(f_8(word))\n",
    "\n",
    "        infix.append(f_9(word))\n",
    "\n",
    "        # can change the scaling constant (third parameter)\n",
    "        bigram.append(f_10(word, bigram_dict, 10000))\n",
    "\n",
    "        most_freq.append(f_11(word))\n",
    "\n",
    "        non_div_alpha.append(f_12(word))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create DataFrame\n",
    "\n",
    "    df = pd.DataFrame({'length': length,\n",
    "                       'num_vowels': v_count,\n",
    "                       'num_conso': c_count,\n",
    "                       'v_div_l': v_div_l,\n",
    "                       'c_div_l': c_div_l,\n",
    "                       'v_div_c': v_div_c,\n",
    "                       'non_alnum': non_alnum,\n",
    "                       'non_alnum_div_l': non_alnum_div_l,\n",
    "                       'digit': digit,\n",
    "                       'digit_l': digit_l,\n",
    "                       'lower': lower,\n",
    "                       'upper': upper,\n",
    "                       'lower_div_l': lower_div_l,\n",
    "                       'upper_div_l': upper_div_l,\n",
    "                       'three_consec_cons': three_consec_cons,\n",
    "                       'alpha_num': alpha_num,\n",
    "                       'six_consec_cons': six_consec_cons,\n",
    "                       'infix': infix,\n",
    "                       'bigram': bigram,\n",
    "                       'most_freq': most_freq,\n",
    "                       'non_div_alpha': non_div_alpha})\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def f_1(word):\n",
    "    \n",
    "    return len(word)\n",
    "\n",
    "def f_2(word):\n",
    "    l = len(word)\n",
    "    vowels = 'aeiou'\n",
    "    cons = 'bcdfghjklmnpqrstvwxyz'\n",
    "    v_count = 0\n",
    "    c_count = 0\n",
    "    \n",
    "    for c in word:\n",
    "        if c in vowels:\n",
    "            v_count += 1\n",
    "        elif c in cons:\n",
    "            c_count += 1\n",
    "\n",
    "\n",
    "    if c_count == 0:\n",
    "        return (v_count, c_count, v_count/l, c_count/l, 0)\n",
    "\n",
    "    return (v_count, c_count, v_count/l, c_count/l, v_count/c_count)\n",
    "\n",
    "def f_3(word):\n",
    "    l = len(word)\n",
    "    non_alnum = 0\n",
    "\n",
    "    for c in word:\n",
    "        if not c.isalnum():\n",
    "            non_alnum += 1\n",
    "\n",
    "    return (non_alnum, non_alnum/l)\n",
    "\n",
    "def f_4(word):\n",
    "    l = len(word)\n",
    "    digit = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isdigit():\n",
    "            digit += 1\n",
    "    return (digit, digit/l)\n",
    "\n",
    "def f_5(word):\n",
    "    l = len(word)\n",
    "    upper = 0 \n",
    "    lower = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isupper():\n",
    "            upper += 1\n",
    "        elif c.islower():\n",
    "            lower += 1 \n",
    "\n",
    "    return (lower, upper, lower/l, upper/l)\n",
    "\n",
    "def f_6(word):\n",
    "    l = len(word)\n",
    "    groups = groupby(word)\n",
    "    result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "\n",
    "    max_count = float('-inf')\n",
    "    for word_count in result:\n",
    "        if word_count[1] > max_count:\n",
    "            max_count = word_count[1]\n",
    "\n",
    "    if max_count >= 3:\n",
    "        return max_count/l\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_7(word):\n",
    "    l = len(word)\n",
    "    alnum = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isalnum():\n",
    "            alnum += 1\n",
    "    \n",
    "    non_alnum = l - alnum\n",
    "\n",
    "    if non_alnum > alnum:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_8(word):\n",
    "    cons = 'bcdfghjklmnpqrstvwxyz'\n",
    "    consec_cons = 0\n",
    "    max_count = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c in cons:\n",
    "            consec_cons += 1\n",
    "        else:\n",
    "            if max_count < consec_cons:\n",
    "                max_count = consec_cons\n",
    "            consec_cons = 0\n",
    "    if max_count == 0:\n",
    "        max_count = consec_cons\n",
    "\n",
    "    if max_count >= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_9(word):\n",
    "    infix = word[1:-1]\n",
    "    non_alnum = 0\n",
    "    \n",
    "    for c in infix:\n",
    "        if not c.isalnum():\n",
    "            non_alnum += 1\n",
    "    if non_alnum >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_10(word, bigram_dict, c = 10000):\n",
    "\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    naturalness = 0\n",
    "    for i in range(len(word)-1):\n",
    "        count += 1.0\n",
    "        naturalness += bigram_dict[(word[i], word[i+1])] / c\n",
    "\n",
    "    if count == 0.0:\n",
    "        return 0\n",
    "    return naturalness / count\n",
    "\n",
    "# return frequency of most frequent symbol\n",
    "def f_11(word):\n",
    "    l = len(word)\n",
    "    most_freq = collections.Counter(word).most_common(1)[0][1]\n",
    "\n",
    "    if most_freq >= 3:\n",
    "        return most_freq/l\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def f_12(word):\n",
    "    l = len(word)\n",
    "    alpha = 0\n",
    "\n",
    "    for c in word:\n",
    "        if c.isalpha():\n",
    "            alpha += 1\n",
    "\n",
    "    non_alpha = l - alpha\n",
    "    if alpha == 0:\n",
    "        return 0\n",
    "    \n",
    "    return non_alpha / alpha\n",
    "\n",
    "def compute_bigram():\n",
    "    \n",
    "    bigram_dict = defaultdict(int)\n",
    "    truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "    for file in truth_files_list:\n",
    "        with open(file) as fd:\n",
    "            for line in fd:\n",
    "                each_line = line.strip().split()\n",
    "                for word in each_line:\n",
    "                    word = word.lower()\n",
    "                    for i in range(len(word)-1):\n",
    "                        bigram_dict[(word[i], word[i+1])] += 1\n",
    "\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Main SVM calling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221504\n",
      "221504\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC']\n",
      "['communlcatlons', 'network.', 'Member', 'companles', 'are', 'strongly', 'encouraged', 'to', 'provlde', 'thls', 'needed', 'support.', 'The', 'state', 'advocacy', 'program\"', '1nclud1ng', 'the', 'new', 'CMA/LINC']\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "['1', 'they', '15', 'to', 'that', 'Natlonal', 'Whltson', 'the', 'energyiconsumlng', 'from']\n",
      "['12', 'they', 'is', 'to', 'that', 'National', 'Whitson', 'the', 'energy-consuming', 'from']\n",
      "[0, 1, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "['than', 'dutles', 'accord', \"Victim's\", 'of', '1n', 'the', 'for', 'by', '7']\n",
      "['than', 'duties', 'accord', \"Victim's\", 'of', 'in', 'the', 'for', 'by', '-']\n",
      "[1, 0, 1, 1, 1, 0, 1, 1, 1, 0]\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "train_data, test_data, train_label, test_label, ground_truth_train, ground_truth_test = div_train(pair, label)\n",
    "\n",
    "# uncomment to test for truth, tesseract pair\n",
    "\n",
    "print(train_data[:10])\n",
    "print(ground_truth_train[:10])\n",
    "print(train_label[:10])\n",
    "\n",
    "print(test_data[:10])\n",
    "print(ground_truth_test[:10])\n",
    "print(test_label[:10])\n",
    "\n",
    "\n",
    "\n",
    "bigram_dict = compute_bigram()\n",
    "featureMatrix_train = buildFeatures(train_data, bigram_dict)\n",
    "featureMatrix_test = buildFeatures(test_data, bigram_dict)\n",
    "\n",
    "# uncomment for testing\n",
    "'''\n",
    "head = featureMatrix_train.head()\n",
    "print(head.to_string())\n",
    "'''\n",
    "\n",
    "# build classifier\n",
    "svm_class = SVC(kernel='rbf', verbose=True, gamma='scale')\n",
    "svm_class.fit(featureMatrix_train, train_label)\n",
    "\n",
    "# prediction\n",
    "prediction = svm_class.predict(featureMatrix_test)\n",
    "\n",
    "output = pd.DataFrame({'data': test_data,\n",
    "                       'label': prediction})\n",
    "\n",
    "print(output[:20])\n",
    "\n",
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# print(confusion_matrix(test_label, prediction))\n",
    "print(classification_report(test_label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Output OCR to detected_typo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv('../output/detected_typo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import detected typo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clean detected typo (remove punctuation & number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      $50,000.\n",
       "1     1nclud1ng\n",
       "3           29,\n",
       "7          thls\n",
       "10    polltlcal\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_typo_and_correct = pd.read_csv('../output/detected_typo.csv',index_col = 0)\n",
    "# remove label column\n",
    "detected_typo = detected_typo_and_correct[detected_typo_and_correct.label == 0].data\n",
    "detected_typo_and_correct = detected_typo_and_correct.data\n",
    "detected_typo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punct_num(series):\n",
    "    result = series.replace(r'\\d','')\n",
    "    result = result.str.extract(r'([a-zA-Z]+)').dropna()[0]\n",
    "    result = result.str.lower()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_typo = remove_punct_num(detected_typo)\n",
    "cleaned_typo_and_correct = remove_punct_num(detected_typo_and_correct)\n",
    "# detected_typo_and_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair, words, label = labelTesseract()\n",
    "\n",
    "true_typo = pd.DataFrame(pair)\n",
    "true_typo.columns = ['correct','typo']\n",
    "for col in true_typo.columns:\n",
    "    true_typo[col] = remove_punct_num(true_typo[col])\n",
    "true_typo = true_typo[true_typo['correct'] != true_typo['typo']].dropna().reset_index(drop = True)\n",
    "true_typo.drop_duplicates(keep = 'first',inplace = True)\n",
    "true_typo = true_typo[['typo','correct']].reset_index(drop = True)\n",
    "# true_typo.set_index('typo',inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define N & V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "truth_counts = 0\n",
    "training = []\n",
    "# create a list of all .txt files\n",
    "truth_files_list = glob.glob('../data/ground_truth/*.txt')\n",
    "# reading the ground truth file\n",
    "for file in truth_files_list:\n",
    "    with open(file) as fd:\n",
    "        for line in fd:\n",
    "            each_line = re.findall(r\"[\\w']+\",line)\n",
    "            for word in each_line:\n",
    "                training.append(word)\n",
    "                truth_counts += 1\n",
    "                \n",
    "training = pd.Series(training)\n",
    "training = training.str.replace(r'\\d','').dropna()\n",
    "\n",
    "training = training.str.lower()\n",
    "training = training[training != '']\n",
    "corpus = training.unique()\n",
    "\n",
    "N = len(training)\n",
    "V = len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Find Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set edit distance to be 1 and find candidates from all ground truth articles. There are 4 different situations:\n",
    "\n",
    "- Insertion\n",
    "- Deletion\n",
    "- Reversal\n",
    "- Substitution\n",
    "\n",
    "Because of the assumption in paper (there are only one typo in each word), we only find candidates that has one edit distance with the typo (there is a special case for reversal, since for the reversal case, edit distance is 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "from nltk import edit_distance\n",
    "\n",
    "def typo_classification(typo,correct):\n",
    "    if (len(typo) > len(correct)):\n",
    "        return 'insertion'\n",
    "    elif (len(typo) < len(correct)):\n",
    "        return 'deletion'\n",
    "    else:\n",
    "        typo_count = Counter(typo)\n",
    "        correct_count = Counter(correct)\n",
    "        if typo_count == correct_count:\n",
    "            return 'reversal'\n",
    "        else:\n",
    "            return 'substitution'\n",
    "\n",
    "def find_candidates(typo,corpus):\n",
    "    candidates = []\n",
    "    candi_type = []\n",
    "    for word in corpus:\n",
    "        ed = edit_distance(typo,word)\n",
    "        word_type = typo_classification(typo,word)\n",
    "#         if len(typo) > 4:\n",
    "#             if ed in [1,2]:\n",
    "#                 candidates.append(word)\n",
    "#                 candi_type.append(word_type)\n",
    "#         else:\n",
    "        if ((ed == 1) |((ed == 2) & (word_type == 'reversal'))):\n",
    "            candidates.append(word)\n",
    "            candi_type.append(word_type)\n",
    "    return candidates,candi_type\n",
    "\n",
    "def find_position(typo,candidates):\n",
    "    position = []\n",
    "    for corr in candidates:\n",
    "        typo_type = typo_classification(typo,corr)\n",
    "        \n",
    "        if (typo_type == 'deletion'):\n",
    "            typo += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    if corr[i] != corr[i-1]:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        typo = typo[:-1]\n",
    "                        position.append([typo,corr,\"#\",corr[i],i,typo_type])\n",
    "                        position.append([typo,corr,\"#\",corr[i],i-1,typo_type])\n",
    "                        break\n",
    "                        \n",
    "                i += 1\n",
    "        elif (typo_type == 'insertion'):\n",
    "            corr += '#'\n",
    "\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    \n",
    "                    if typo[i] != typo[i-1]:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        break\n",
    "                    elif ((typo[i] == typo[i-1])& (typo[i] == typo[i-2])):\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-2,typo_type])\n",
    "                        break\n",
    "                    else:\n",
    "                        corr = corr[:-1]\n",
    "                        position.append([typo,corr,typo[i],\"#\",i,typo_type])\n",
    "                        position.append([typo,corr,typo[i],\"#\",i-1,typo_type])\n",
    "                        break\n",
    "                i += 1\n",
    "        elif (typo_type == 'substitution'):\n",
    "            i = 0\n",
    "            while i < len(corr):\n",
    "                if (corr[i] != typo[i]):\n",
    "                    position.append([typo,corr,typo[i],corr[i],i,typo_type])\n",
    "                    break\n",
    "                i+=1\n",
    "                \n",
    "        elif (typo_type == 'reversal'):\n",
    "            i = 0\n",
    "            while i < len(corr)-1:\n",
    "                if ((typo[i] == corr[i+1]) & (typo[i+1] == corr[i])):\n",
    "                    typo_comb = typo[i] + typo[i+1]\n",
    "                    position.append([typo,corr,typo_comb,typo_comb[::-1],i,typo_type])\n",
    "                    break\n",
    "                i +=1\n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Import 4 confusion matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionsub=pd.read_csv('../data/confusion_matrix/sub_matrix.csv',index_col = 0)\n",
    "confusionadd=pd.read_csv('../data/confusion_matrix/add_matrix.csv',index_col = 0)\n",
    "confusiondel=pd.read_csv('../data/confusion_matrix/del_matrix.csv',index_col = 0)\n",
    "confusionrev=pd.read_csv('../data/confusion_matrix/rev_matrix.csv',index_col = 0) \n",
    "# corpus = set(truth_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Count bigram & 1gram & freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating word(character) frequency for ground truth paper for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "def bigram(string):\n",
    "    x = []\n",
    "    for i in range(len(string)):\n",
    "        if i == len(string) - 1:\n",
    "            return x\n",
    "        else:\n",
    "            x.append(string[i] + string[i+1])\n",
    "            \n",
    "def one_gram(string):\n",
    "    return list(string)\n",
    "\n",
    "def total_freq(training,types):\n",
    "    if types == 'bigram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += bigram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'onegram':\n",
    "        result = []\n",
    "        for string in training:\n",
    "            result += one_gram(string)\n",
    "        return Counter(result)\n",
    "    elif types == 'freq':\n",
    "        return Counter(training)\n",
    "    \n",
    "total_freq_bigram = total = total_freq(training,types = 'bigram')\n",
    "total_freq_1gram = total = total_freq(training,types = 'onegram')\n",
    "total_freq = total = total_freq(training,types = 'freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Calculate Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction = pd.DataFrame()\n",
    "\n",
    "def probabilityfunction(correction):\n",
    "    for i in range(0,correction.shape[0]):\n",
    "        typo = correction.iloc[i,0]\n",
    "        index=correction.iloc[i,4]\n",
    "        specificword=correction.iloc[i,1]\n",
    "        if correction.iloc[i,5]=='insertion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=typo[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result =add/total\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                add =confusionadd.loc[X,Y]\n",
    "                total=len(training)\n",
    "\n",
    "                result=add/total\n",
    "\n",
    "        if correction.iloc[i,5]=='deletion':\n",
    "            if index != 0:\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index-1]\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                    #lis.append(total)\n",
    "                result=delt/total\n",
    "\n",
    "\n",
    "            if index == 0:\n",
    "                X='#'\n",
    "                Y=specificword[index]\n",
    "                delt=confusiondel.loc[X,Y]\n",
    "                totall=len(training)\n",
    "\n",
    "                result=delt/totall\n",
    "        if correction.iloc[i,5]=='reversal':\n",
    "\n",
    "\n",
    "                #index=correction.iloc[i,4]\n",
    "                X=specificword[index]\n",
    "                Y=specificword[index+1]\n",
    "                rev=confusionrev.loc[X,Y]\n",
    "                \n",
    "                total = total_freq_bigram[X+Y]\n",
    "                result=rev/total\n",
    "\n",
    "\n",
    "        if correction.iloc[i,5]=='substitution':\n",
    "            X=correction.iloc[i,2]\n",
    "            Y=correction.iloc[i,3]\n",
    "            sub = confusionsub.loc[X,Y]\n",
    "\n",
    "            total = total_freq_1gram[Y]\n",
    "                #lis.append(total)\n",
    "            result=sub/total\n",
    "            \n",
    "        correction.loc[i,'probability of t given c'] = result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Calculate Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correction(typos):\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "    output = []\n",
    "    no_correction = 0\n",
    "    no_correct_word = []\n",
    "\n",
    "    for typo in tqdm_notebook(typos):\n",
    "        try:\n",
    "            candidates,cand_type = find_candidates(typo,corpus)\n",
    "            correction = find_position(typo,candidates)\n",
    "            correction = pd.DataFrame(correction)\n",
    "\n",
    "            if correction.empty:  \n",
    "                output.append(typo)\n",
    "                no_correct_word.append(typo)\n",
    "                no_correction += 1\n",
    "\n",
    "            else:\n",
    "                correction.columns = ['Typo','Correction','old','new','index','type']\n",
    "                correction = correction[correction['index'] >= 0]\n",
    "\n",
    "                if len(correction) == 1:\n",
    "                    output.append(correction.loc[0,'Correction'])\n",
    "                else:\n",
    "                    # 1. calculate the prior\n",
    "\n",
    "                    freq = [] # the number of times that the proposed correction c appears in the training set\n",
    "                    for cor in correction['Correction']:\n",
    "                        freq.append(total_freq[cor])    \n",
    "\n",
    "                    N = len(training)\n",
    "                    V = len(corpus)\n",
    "\n",
    "                    prior = (pd.DataFrame(freq) + 0.5)/(N + V/2)\n",
    "\n",
    "                    correction['probability of c'] = prior\n",
    "\n",
    "                    probabilityfunction(correction)\n",
    "\n",
    "                    # 3. Calculate the posterior and find the correction that has maximum posterior\n",
    "\n",
    "                    correction['posterior'] = correction['probability of c'] * correction['probability of t given c']\n",
    "                    best = correction[correction.posterior == correction.posterior.max()].Correction.values[0]\n",
    "                    output.append(best)\n",
    "        except:\n",
    "            output.append(typo)\n",
    "            no_correct_word.append(typo)\n",
    "            no_correction += 1\n",
    "    \n",
    "    return (pd.Series(output)),no_correction,no_correct_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_denominator = len(cleaned_typo)\n",
    "recall_denominator = len(cleaned_typo_and_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def vintersection(list1,list2,ngram = False):\n",
    "    list1_dict = {}\n",
    "    list2_dict = {}\n",
    "    \n",
    "    if ngram:\n",
    "        list1 = list(''.join(list1))\n",
    "        list2 = list(''.join(list2))\n",
    "\n",
    "    for i in list1:\n",
    "        list1_dict[i] = list1_dict.get(i,0) + 1\n",
    "\n",
    "    for i in list2:\n",
    "        list2_dict[i] = list2_dict.get(i,0) + 1\n",
    "        \n",
    "    result = {}\n",
    "    for key in list1_dict.keys():\n",
    "        if key in list2_dict.keys():\n",
    "            value1 = list1_dict[key]\n",
    "            value2 = list2_dict[key]\n",
    "            min_value = min(value1,value2)\n",
    "            result[key] = min_value\n",
    "    return sum(result.values())\n",
    "\n",
    "def precision(GT,OCR,ngram = False):\n",
    "    TP = vintersection(GT,OCR,ngram)\n",
    "    if ngram:\n",
    "        OCR = list(''.join(OCR))\n",
    "    return TP/precision_denominator\n",
    "\n",
    "def recall(GT,OCR,ngram = False):\n",
    "    TP = vintersection(GT,OCR)\n",
    "    if ngram:\n",
    "        GT = list(''.join(GT))\n",
    "    return TP/recall_denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Case 1: Correct all typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d142cdd6f9d24ccda4f2c02602ffb6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12111), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "typos = true_typo['typo']\n",
    "correct = true_typo['correct']\n",
    "\n",
    "Correction_output_all,no_correction_num,no_correct_word = Correction(typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.20%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(vintersection(Correction_output_all,correct)/len(Correction_output_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrected rate: 39.3%\n"
     ]
    }
   ],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output correction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correction_output_all.to_csv('../output/Correction_output_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall & precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(Correction_output,correct[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('communications', 'communlcatlons'),\n",
       " ('network.', 'network.'),\n",
       " ('Member', 'Member'),\n",
       " ('companies', 'companles'),\n",
       " ('are', 'are'),\n",
       " ('strongly', 'strongly'),\n",
       " ('encouraged', 'encouraged'),\n",
       " ('to', 'to'),\n",
       " ('provide', 'provlde'),\n",
       " ('this', 'thls'),\n",
       " ('needed', 'needed'),\n",
       " ('support.', 'support.'),\n",
       " ('The', 'The'),\n",
       " ('state', 'state'),\n",
       " ('advocacy', 'advocacy'),\n",
       " ('program*', 'program\"'),\n",
       " ('including', '1nclud1ng'),\n",
       " ('the', 'the'),\n",
       " ('new', 'new'),\n",
       " ('CMA/LINC', 'CMA/LINC'),\n",
       " ('computer', 'computer'),\n",
       " ('network,', 'network.'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('heavily', 'heavlly'),\n",
       " ('involved', '1nvolved'),\n",
       " ('in', '1n'),\n",
       " ('1986', '1995'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('critical', 'crltlcal'),\n",
       " ('environmental', 'envlronmental'),\n",
       " ('issues', 'lssues'),\n",
       " ('identified', '1dent1Â£1ed'),\n",
       " ('by', 'by'),\n",
       " ('the', 'the'),\n",
       " ('National', 'Natlonal'),\n",
       " ('Conference', 'Conference'),\n",
       " ('of', 'of'),\n",
       " ('State', 'State'),\n",
       " ('Legislators,', 'Leglslators,'),\n",
       " ('namely,', 'namely,'),\n",
       " ('groundwater', 'groundwater'),\n",
       " ('and', 'and'),\n",
       " ('the', 'the'),\n",
       " ('disposal', 'dlsposal'),\n",
       " ('of', 'of'),\n",
       " ('hazardous', 'hazardous'),\n",
       " ('wastes.', 'wastes.'),\n",
       " ('Of', 'of'),\n",
       " ('course,', 'course,'),\n",
       " ('other', 'other'),\n",
       " ('issues', 'lssues'),\n",
       " ('will', 'Hill'),\n",
       " ('continue', 'contlnue'),\n",
       " ('to', 'm'),\n",
       " ('be', 'be'),\n",
       " ('of', 'of'),\n",
       " ('major', 'major'),\n",
       " ('concern', 'concern'),\n",
       " ('such', 'such'),\n",
       " ('as', 'as'),\n",
       " ('state', 'state'),\n",
       " ('superfunds', 'Superfund:'),\n",
       " ('which', 'whlch'),\n",
       " ('are', 'are'),\n",
       " ('currently', 'currently'),\n",
       " ('under', 'under'),\n",
       " ('detailed', 'detalled'),\n",
       " ('study', 'study'),\n",
       " ('by', 'by'),\n",
       " ('the', 'the'),\n",
       " ('CMA', 'cm'),\n",
       " ('State', 'State'),\n",
       " ('Hazardous', 'Hazardous'),\n",
       " ('Waste/Groundwater', 'Waste/Groundwater'),\n",
       " ('Task', 'Task'),\n",
       " ('Group.', 'Group.'),\n",
       " ('State', 'State'),\n",
       " ('right-to-know', 'rlghtitoiknow'),\n",
       " ('action', 'actlon'),\n",
       " ('continues', 'contlnues'),\n",
       " ('to', 'to'),\n",
       " ('grow', 'grow'),\n",
       " ('and', 'and'),\n",
       " ('even', 'even'),\n",
       " ('expand', 'expand'),\n",
       " ('in', '1n'),\n",
       " ('scope.', 'scope.'),\n",
       " ('New', 'New'),\n",
       " (\"Jersey's\", \"Jersey's\"),\n",
       " ('\"Toxic', '\"Toxlc'),\n",
       " ('Catastrophe', 'Catastrophe'),\n",
       " ('Prevention', 'Preventlon'),\n",
       " ('Act\",', 'Act\",'),\n",
       " ('signed', 'Signed'),\n",
       " ('in', '1n'),\n",
       " ('early', 'early'),\n",
       " ('January,', 'January,'),\n",
       " ('not', 'not'),\n",
       " ('only', 'only'),\n",
       " ('requires', 'requlres'),\n",
       " ('the', 'the'),\n",
       " ('reporting', 'reportlng'),\n",
       " ('of', 'of'),\n",
       " ('information', '1nformatlon'),\n",
       " ('concerning', 'concernlng'),\n",
       " (\"industry's\", \"1ndustry's\"),\n",
       " ('operations', 'operatlons'),\n",
       " ('but', 'but'),\n",
       " ('also', 'also'),\n",
       " ('allows', 'allows'),\n",
       " ('the', 'the'),\n",
       " ('state', 'state'),\n",
       " ('to', 'to'),\n",
       " ('order', 'order'),\n",
       " ('risk', 'rlsk'),\n",
       " ('reduction', 'reductlon'),\n",
       " ('measures', 'measures'),\n",
       " ('or', 'or'),\n",
       " ('even', 'even'),\n",
       " ('order', 'order'),\n",
       " ('the', 'the'),\n",
       " ('cessation', 'cessatlon'),\n",
       " ('of', 'of'),\n",
       " ('operations.', 'operatlons.'),\n",
       " ('Toxic', 'Toxlc'),\n",
       " ('air', 'air'),\n",
       " ('will', 'will'),\n",
       " ('become', 'become'),\n",
       " ('even', 'even'),\n",
       " ('more', 'more'),\n",
       " ('important', 'unportant'),\n",
       " ('at', 'at'),\n",
       " ('the', 'the'),\n",
       " ('state', 'state'),\n",
       " ('level', 'level'),\n",
       " ('as', 'as'),\n",
       " ('Federal', 'Federal'),\n",
       " ('EPA', 'EPA'),\n",
       " ('pushes', 'pushes'),\n",
       " ('this', 'thls'),\n",
       " ('issue', 'lssue'),\n",
       " ('along', 'along'),\n",
       " ('with', 'Hlth'),\n",
       " ('groundwater', 'groundwater'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('states.', 'states.'),\n",
       " (\"CMA's\", \"CMA'S\"),\n",
       " ('State', 'State'),\n",
       " ('Affairs', 'Affalrs'),\n",
       " ('program', 'program'),\n",
       " ('has', 'has'),\n",
       " ('been', 'been'),\n",
       " ('an', 'an'),\n",
       " ('active', 'actlve'),\n",
       " ('participant', 'partlclpant'),\n",
       " ('in', '1n'),\n",
       " (\"CMA's\", \"CMA'S\"),\n",
       " ('development', 'development'),\n",
       " ('of', 'of'),\n",
       " ('an', 'an'),\n",
       " ('air', 'alr'),\n",
       " ('toxics', 'toxlcs'),\n",
       " ('policy.', 'pollcy.'),\n",
       " ('A', 'A'),\n",
       " ('new,', 'new,'),\n",
       " ('but', 'but'),\n",
       " ('rapidly', 'rapldly'),\n",
       " ('growing', 'growlng'),\n",
       " ('issue', 'lssue'),\n",
       " ('at', 'at'),\n",
       " ('the', 'the'),\n",
       " ('state', 'state'),\n",
       " ('and', 'and'),\n",
       " ('federal', 'federal'),\n",
       " ('level', 'level'),\n",
       " ('is', '15'),\n",
       " ('liability', 'liability'),\n",
       " ('insurance', '1nsurance'),\n",
       " ('and', 'and'),\n",
       " ('the', 'the'),\n",
       " ('related', 'related'),\n",
       " ('issue', 'issue'),\n",
       " ('of', 'of'),\n",
       " ('tort', 'tort'),\n",
       " ('reform.', 'reform.'),\n",
       " (\"CMA's\", \"CMA'S\"),\n",
       " ('State', 'State'),\n",
       " ('Affairs', 'Affalrs'),\n",
       " ('program', 'program'),\n",
       " ('is', '15'),\n",
       " ('actively', 'actlvely'),\n",
       " ('participating', 'partlclpatlng'),\n",
       " ('in', '1n'),\n",
       " ('a', '3'),\n",
       " ('large', 'large'),\n",
       " ('coalition', 'coalltlon'),\n",
       " ('of', 'of'),\n",
       " ('interested', '1nterested'),\n",
       " ('trade', 'trade'),\n",
       " ('associations', 'assoclatlons'),\n",
       " ('and', 'and'),\n",
       " ('individual', '1nd1v1dual'),\n",
       " ('companies', 'companles'),\n",
       " ('under', 'under'),\n",
       " ('the', 'the'),\n",
       " ('auspices', 'ausplces'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('American', 'Amerlcan'),\n",
       " ('Legislative', 'Leglslatlve'),\n",
       " ('Exchange', 'Exchange'),\n",
       " ('Council.', 'Councll.'),\n",
       " ('Nearly', 'Nearly'),\n",
       " ('all', 'all'),\n",
       " ('states', 'states'),\n",
       " ('are', 'are'),\n",
       " ('involved', '1nvolved'),\n",
       " ('in', '1n'),\n",
       " ('some', 'some'),\n",
       " ('aspect', 'aspect'),\n",
       " ('of', 'of'),\n",
       " ('this', 'thls'),\n",
       " ('issue.', 'issue.'),\n",
       " ('Of', 'of'),\n",
       " ('course,', 'course,'),\n",
       " ('hazardous', 'hazardous'),\n",
       " ('material', 'materlal'),\n",
       " ('transportation,', 'transportatlon,'),\n",
       " ('public', 'publlc'),\n",
       " ('compensation', 'compensatlon'),\n",
       " ('and', 'and'),\n",
       " ('chronic', 'chronlc'),\n",
       " ('health', 'health'),\n",
       " ('issues', 'lssues'),\n",
       " ('will', 'Hill'),\n",
       " ('continue', 'contlnue'),\n",
       " ('to', 'm'),\n",
       " ('be', 'be'),\n",
       " ('active', 'actlve'),\n",
       " ('issues', 'lssues'),\n",
       " ('as', 'as'),\n",
       " ('well', 'well'),\n",
       " ('in', '1n'),\n",
       " ('1986.', '1935.'),\n",
       " ('Media', 'Medla'),\n",
       " ('Page', 'Page'),\n",
       " ('3', '3'),\n",
       " ('CMA', 'cm'),\n",
       " ('038607', '039507'),\n",
       " ('The', 'The'),\n",
       " ('closing', 'closmg'),\n",
       " ('months', 'months'),\n",
       " ('of', 'of'),\n",
       " ('1985', '1995'),\n",
       " ('saw', 'saw'),\n",
       " ('concentrated', 'concentrated'),\n",
       " ('media', 'medla'),\n",
       " ('attention', 'attentlon'),\n",
       " ('on', 'on'),\n",
       " ('the', 'the'),\n",
       " ('subject', 'subject'),\n",
       " ('of', 'of'),\n",
       " ('air', 'air'),\n",
       " ('toxics.', 'toxlcs.'),\n",
       " ('The', 'The'),\n",
       " ('first', 'flrst'),\n",
       " ('anniversary', 'annlversary'),\n",
       " ('of', 'of'),\n",
       " ('Bhopal', 'Bhopal'),\n",
       " ('and', 'and'),\n",
       " ('related', 'related'),\n",
       " ('activities', 'actlvltles'),\n",
       " ('of', 'of'),\n",
       " ('environmental', 'envlronmental'),\n",
       " ('groups,', 'groups,'),\n",
       " ('the', 'the'),\n",
       " ('EPA', 'EPA'),\n",
       " ('hazardous', 'hazardous'),\n",
       " ('chemicals', 'chemlcals'),\n",
       " ('listing', '115mm;'),\n",
       " ('and,', 'and.'),\n",
       " ('in', '1n'),\n",
       " ('depth', 'depth'),\n",
       " ('media', 'medla'),\n",
       " ('analysis,', 'analysls,'),\n",
       " ('made', 'made'),\n",
       " ('headlines', 'headllnes'),\n",
       " ('at', 'at'),\n",
       " ('the', 'the'),\n",
       " ('same', 'same'),\n",
       " ('time.', 'tune.'),\n",
       " ('Reauthori2ation', 'Reauthor12atlon'),\n",
       " ('of', 'of'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('probably', 'probably'),\n",
       " ('gained', 'galned'),\n",
       " ('more', 'more'),\n",
       " ('media', 'medla'),\n",
       " ('attention', 'attentlon'),\n",
       " ('than', 'than'),\n",
       " ('expected,', 'expected,'),\n",
       " ('primarily', 'prlmarlly'),\n",
       " ('because', 'because'),\n",
       " ('of', 'of'),\n",
       " ('its', '1:5'),\n",
       " ('intrusion', '1ntruslon'),\n",
       " ('into', '1nto'),\n",
       " ('the', 'the'),\n",
       " ('budget', 'budget'),\n",
       " ('reconciliation', 'reconclllatlon'),\n",
       " ('process.', 'process.'),\n",
       " ('Most', 'Most'),\n",
       " ('of', 'of'),\n",
       " ('this', 'thls'),\n",
       " ('attention', 'attentlon'),\n",
       " ('was', 'was'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('form', 'fom'),\n",
       " ('of', 'of'),\n",
       " ('news', 'news'),\n",
       " ('rather', 'rather'),\n",
       " ('than', 'than'),\n",
       " ('editorial', 'edltorlal'),\n",
       " ('coverage.', 'coverage.'),\n",
       " ('1985', '1995'),\n",
       " ('-', '7'),\n",
       " ('as', 'as'),\n",
       " ('might', 'mlght'),\n",
       " ('have', 'have'),\n",
       " ('been', 'been'),\n",
       " ('anticipated', 'antlclpated'),\n",
       " ('following', 'followlng'),\n",
       " ('the', 'the'),\n",
       " ('December', 'December'),\n",
       " ('1984', '1994'),\n",
       " ('Bhopal', 'Bhopal'),\n",
       " ('accident', 'accldent'),\n",
       " ('-', '7'),\n",
       " ('became', 'became'),\n",
       " ('a', 'a'),\n",
       " ('year', 'year'),\n",
       " ('of', 'of'),\n",
       " ('intense', '1ntense'),\n",
       " ('environmental', 'envlronmental'),\n",
       " ('inquiry.', 'inquiry.'),\n",
       " ('Initial', 'Inltlal'),\n",
       " ('interest', '1nterest'),\n",
       " ('was', 'was'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('potential', 'potentlal'),\n",
       " ('for', 'for'),\n",
       " ('acute', 'acute'),\n",
       " ('incidents', '1nc1dents'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('United', 'Unlted'),\n",
       " ('States.', 'States.'),\n",
       " ('Stories', 'Storles'),\n",
       " ('focused', 'focused'),\n",
       " ('on', 'on'),\n",
       " ('questions', 'questlons'),\n",
       " ('of', 'of'),\n",
       " ('failure', '{allure'),\n",
       " ('to', 'to'),\n",
       " ('regulate', 'regulate'),\n",
       " ('or', 'or'),\n",
       " ('inadequacy', '1nadequacy'),\n",
       " ('of', 'of'),\n",
       " ('existing', 'exlstlng'),\n",
       " ('law.', 'law.'),\n",
       " ('For', 'For'),\n",
       " ('the', 'the'),\n",
       " ('media,', 'medla,'),\n",
       " ('the', 'the'),\n",
       " ('Institute,', 'Instltute,'),\n",
       " ('W.V.', 'w.v.'),\n",
       " ('release', 'release'),\n",
       " ('answered', 'answered'),\n",
       " ('the', 'the'),\n",
       " ('question', 'questlon'),\n",
       " ('\"can', '\"can'),\n",
       " ('it', '1:'),\n",
       " ('happen', 'happen'),\n",
       " ('here?\"', 'here?\"'),\n",
       " ('Attention', 'Attentlon'),\n",
       " ('to', 'to'),\n",
       " ('\"acute\"', '\"acute\"'),\n",
       " ('threats', 'threats'),\n",
       " ('accelerated', 'accelerated'),\n",
       " ('in', '1n'),\n",
       " ('their', 'thelr'),\n",
       " ('Bhopal', 'Bhopal'),\n",
       " ('retrospective.', 'retrospectlve.'),\n",
       " ('Media', 'Medla'),\n",
       " ('from', 'from'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('states', 'states'),\n",
       " ('such', 'such'),\n",
       " ('as', 'as'),\n",
       " ('Texas,', 'Texas,'),\n",
       " ('Louisiana,', 'Louisiana,'),\n",
       " ('west', 'west'),\n",
       " ('Virginia,', 'Virginia,'),\n",
       " ('Illinois,', 'Illlnols,'),\n",
       " ('Ohio', 'Dhlo'),\n",
       " ('and', 'and'),\n",
       " ('Pennsylvania', 'Pennsylvanla'),\n",
       " ('were', 'were'),\n",
       " ('concerned', 'concerned'),\n",
       " ('safety', 'safety'),\n",
       " ('and', 'and'),\n",
       " ('design.', 'deslgn.'),\n",
       " ('New', 'New'),\n",
       " ('Jersey', 'Jersey'),\n",
       " ('attention', 'attentlon'),\n",
       " ('included', '1ncluded'),\n",
       " ('\"chronic\"', '\"chronlc\"'),\n",
       " ('issues', 'lssues'),\n",
       " ('as', 'as'),\n",
       " ('well', 'well'),\n",
       " ('as', 'as'),\n",
       " ('\"acute,\"', '\"acute,\"'),\n",
       " ('raising', 'raising'),\n",
       " ('the', 'the'),\n",
       " ('idea', '1dea'),\n",
       " ('of', 'of'),\n",
       " ('affect', 'affect'),\n",
       " ('on', 'on'),\n",
       " ('communities,', 'communltles,'),\n",
       " ('as', 'as'),\n",
       " ('opposed', 'opposed'),\n",
       " ('to', 'to'),\n",
       " ('workers.', 'workers.'),\n",
       " ('For', 'For'),\n",
       " ('the', 'the'),\n",
       " ('most', 'most'),\n",
       " ('part', 'part'),\n",
       " ('our', 'our'),\n",
       " ('activities', 'actlvltles'),\n",
       " ('relating', 'relatlng'),\n",
       " ('to', 'to'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('were', 'were'),\n",
       " ('proactive.', 'proactlve.'),\n",
       " ('Bhopal/air', 'Bhopal/alr'),\n",
       " ('toxics', 'toxlcs'),\n",
       " ('activities', 'actlvltles'),\n",
       " ('were', 'were'),\n",
       " ('responsive.', 'responslve.'),\n",
       " ('The', 'The'),\n",
       " ('national', 'natlonal'),\n",
       " ('media', 'medla'),\n",
       " ('appeared', 'appeared'),\n",
       " ('to', 'to'),\n",
       " ('understand', 'understand'),\n",
       " ('the', 'the'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('industry', '1ndustry'),\n",
       " ('rationale', 'ratlonale'),\n",
       " ('for', 'for'),\n",
       " ('a', 'a'),\n",
       " ('broad', 'broad'),\n",
       " ('based', 'based'),\n",
       " ('tax', 'tax'),\n",
       " ('and', 'and'),\n",
       " ('generally', 'generally'),\n",
       " ('referred', 'referred'),\n",
       " ('to', 'to'),\n",
       " ('that', 'that'),\n",
       " ('position', 'posltlon'),\n",
       " ('in', '1n'),\n",
       " ('their', 'thelr'),\n",
       " ('coverage,', 'coverage,'),\n",
       " ('in', '1n'),\n",
       " ('addition,', 'addltlon.'),\n",
       " ('those', 'those'),\n",
       " ('reporters', 'reporters'),\n",
       " ('who', 'who'),\n",
       " ('have', 'have'),\n",
       " ('covered', 'covered'),\n",
       " ('the', 'the'),\n",
       " ('story', 'story'),\n",
       " ('over', 'over'),\n",
       " ('a', 'a'),\n",
       " ('period', 'perlod'),\n",
       " ('of', 'of'),\n",
       " ('time', 'time'),\n",
       " ('agree', 'agree'),\n",
       " ('with', 'mm'),\n",
       " ('the', 'the'),\n",
       " ('industry', '1ndustry'),\n",
       " ('contention', 'contentlon'),\n",
       " ('that', 'that'),\n",
       " ('a', 'a'),\n",
       " ('waste', 'waste'),\n",
       " ('generation', 'generatlon'),\n",
       " ('tax', 'tax'),\n",
       " ('bears', 'bears'),\n",
       " ('little', 'llttle'),\n",
       " ('relationship', 'relatlonshlp'),\n",
       " ('to', 'to'),\n",
       " ('hazardous', 'hazardous'),\n",
       " ('waste', 'waste'),\n",
       " ('sites.', 'sltes.'),\n",
       " ('Courts', 'Courts'),\n",
       " ('and', 'and'),\n",
       " ('Agency', 'Agency'),\n",
       " ('Decisions', 'Dealslons'),\n",
       " ('Federal', 'Federal'),\n",
       " ('Maritime', 'Marltlme'),\n",
       " ('Commission.', 'Commlsslon.'),\n",
       " ('A', 'A'),\n",
       " ('victory', 'Vlctory'),\n",
       " ('I', 'I'),\n",
       " ('CMA,', 'am,'),\n",
       " ('through', 'through'),\n",
       " ('briefs', 'brlefs'),\n",
       " ('and', 'and'),\n",
       " ('affidavits,', 'affldavlts,'),\n",
       " ('convinced', 'convlnced'),\n",
       " ('an', 'an'),\n",
       " ('Administrative', 'Admlnlstratlve'),\n",
       " ('Law', 'Law'),\n",
       " ('Judge', 'Judge'),\n",
       " ('to', 'to'),\n",
       " ('decide', 'declde'),\n",
       " ('that', 'that'),\n",
       " ('two', 'two'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('parcel', 'parcel'),\n",
       " ('tankers', 'tankers'),\n",
       " ('were', 'were'),\n",
       " ('\"tramps\"', '\"tramps\"'),\n",
       " ('rather', 'rather'),\n",
       " ('than', 'than'),\n",
       " ('\"ocean', '\"ocean'),\n",
       " ('conanon', 'conanon'),\n",
       " ('carriersâ', 'carrlers...'),\n",
       " ('and', 'and'),\n",
       " ('thus', 'thus'),\n",
       " ('exempt', 'exempt'),\n",
       " ('from', 'from'),\n",
       " ('antitrust', 'antltrust'),\n",
       " ('laws.', 'laws.'),\n",
       " ('While', 'Whlle'),\n",
       " ('this', 'thls'),\n",
       " ('decision', 'declslon'),\n",
       " ('is', '15'),\n",
       " ('likely', 'llkely'),\n",
       " ('to', 'to'),\n",
       " ('be', 'be'),\n",
       " ('appealed', 'appealed'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('entire', 'entlre'),\n",
       " ('Commission,', 'Commlsslon.'),\n",
       " ('it', '1:'),\n",
       " ('may', 'may'),\n",
       " ('serve', 'serve'),\n",
       " ('as', 'as'),\n",
       " ('a', 'a'),\n",
       " ('precedent', 'precedent'),\n",
       " ('to', 'to'),\n",
       " ('prevent', 'prevent'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('parcel', 'parcel'),\n",
       " ('tankers', 'tankers'),\n",
       " ('from', 'from'),\n",
       " ('obtaining', 'obtalnlng'),\n",
       " ('antitrust', 'antltrust'),\n",
       " ('exemptions', 'exemptlons'),\n",
       " ('for', 'for'),\n",
       " ('price', 'price'),\n",
       " ('setting', 'settlng'),\n",
       " ('and', 'and'),\n",
       " ('market', 'market'),\n",
       " ('allocation', 'allocatlon'),\n",
       " ('agreements.', 'agreements.'),\n",
       " ('Those', 'Those'),\n",
       " ('activities', 'actlvltles'),\n",
       " ('could', 'could'),\n",
       " ('cost', 'cost'),\n",
       " ('the', 'the'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('industry', '1ndustry'),\n",
       " ('200', '2mm'),\n",
       " ('million', 'mllllon'),\n",
       " ('dollars', 'dollars'),\n",
       " ('per', 'per'),\n",
       " ('year', 'year'),\n",
       " ('in', '1n'),\n",
       " ('increased', '1ncreased'),\n",
       " ('rates', 'rates'),\n",
       " ('on', 'on'),\n",
       " ('bulk', 'bulk'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('exports', 'exports'),\n",
       " ('and', 'and'),\n",
       " ('imports.', 'unports.'),\n",
       " ('Federal', 'Federal'),\n",
       " ('Energy', 'Energy'),\n",
       " ('Regulatory', 'Regulatory'),\n",
       " ('Commission.', 'Commlsslon.'),\n",
       " ('Another', 'Another'),\n",
       " ('victoryl', 'vlctoryl'),\n",
       " ('The', 'The'),\n",
       " ('Federal', 'Federal'),\n",
       " ('Energy', 'Energy'),\n",
       " ('Regulatory', 'Regulatory'),\n",
       " ('Commission', 'Commlsslon'),\n",
       " ('(FERC)', '(mum'),\n",
       " ('has', 'has'),\n",
       " ('approved', 'approved'),\n",
       " ('a', 'a'),\n",
       " ('rule', 'rule'),\n",
       " ('on', 'on'),\n",
       " ('natural', 'natural'),\n",
       " ('gas', 'gas'),\n",
       " ('Page', 'Page'),\n",
       " ('4', '4'),\n",
       " ('CMA', 'cm'),\n",
       " ('038608', 'means'),\n",
       " ('transportation', 'transportatlon'),\n",
       " ('that', 'that'),\n",
       " ('adopts', 'adopts'),\n",
       " ('many', 'many'),\n",
       " ('CMA', 'CMA'),\n",
       " ('recommendations.', 'recommendatlons.'),\n",
       " ('The', 'The'),\n",
       " ('rule', 'rule'),\n",
       " ('(Order', '(Order'),\n",
       " ('No.', 'No.'),\n",
       " ('436)', '4357'),\n",
       " ('is', '15'),\n",
       " ('intended', '1ntended'),\n",
       " ('to', 'to'),\n",
       " ('promote', 'promote'),\n",
       " ('competition', 'competltlon'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('natural', 'natural'),\n",
       " ('gas', 'gas'),\n",
       " ('market', 'market'),\n",
       " ('by', 'by'),\n",
       " ('facilitating', 'facllltatlng'),\n",
       " ('the', 'the'),\n",
       " ('movement', 'movement'),\n",
       " ('of', 'of'),\n",
       " ('gas', 'gas'),\n",
       " ('from', 'from'),\n",
       " ('the', 'the'),\n",
       " ('producer', 'producer'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('user.', 'user.'),\n",
       " ('When', 'When'),\n",
       " ('fully', 'fully'),\n",
       " ('implemented', 'unplemented'),\n",
       " ('by', 'by'),\n",
       " ('the', 'the'),\n",
       " ('pipelines,', 'pipellnes,'),\n",
       " ('there', 'there'),\n",
       " ('will', 'Will'),\n",
       " ('be', 'be'),\n",
       " ('a', 'a'),\n",
       " ('substantial', 'substantlal'),\n",
       " ('cost', 'cost'),\n",
       " ('savings', 'savlngs'),\n",
       " ('to', 'to'),\n",
       " ('industrial', '1ndustrlal'),\n",
       " ('users', 'users'),\n",
       " ('of', 'of'),\n",
       " ('gas.', 'gas.'),\n",
       " ('The', 'The'),\n",
       " ('annual', 'annual'),\n",
       " ('savings', 'savlngs'),\n",
       " ('for', 'for'),\n",
       " ('CMA', 'cm'),\n",
       " ('members', 'members'),\n",
       " ('is', '15'),\n",
       " ('estimated', 'estlmated'),\n",
       " ('at', 'at'),\n",
       " ('$300', '53m'),\n",
       " ('to', 'to'),\n",
       " ('$750', '$750'),\n",
       " ('million.', 'mllllon.'),\n",
       " ('In', 'In'),\n",
       " ('addition,', 'addltlon.'),\n",
       " ('CMA', 'cm'),\n",
       " ('and', 'and'),\n",
       " ('its', '1:5'),\n",
       " ('coalition', 'Coalltlon'),\n",
       " ('partners', 'partners'),\n",
       " ('successfully', 'successfully'),\n",
       " ('argued', 'argued'),\n",
       " ('against', 'agalnst'),\n",
       " ('inclusion', '1ncluslon'),\n",
       " ('of', 'of'),\n",
       " ('a', 'a'),\n",
       " ('new', 'new'),\n",
       " ('block', 'block'),\n",
       " ('billing', 'bllllng'),\n",
       " ('mechanism', 'mechanlsm'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('final', 'flnal'),\n",
       " ('rule.', 'rule.'),\n",
       " ('CMA', 'CMA'),\n",
       " ('testified', 'testlfled'),\n",
       " ('that', 'that'),\n",
       " ('block', 'block'),\n",
       " ('billing', 'bllllng'),\n",
       " ('would', 'would'),\n",
       " ('significantly', 'slgnlflcantly'),\n",
       " ('raise', 'ralse'),\n",
       " ('the', 'the'),\n",
       " ('price', 'price'),\n",
       " ('of', 'of'),\n",
       " ('natural', 'natural'),\n",
       " ('gas', 'gas'),\n",
       " ('for', 'for'),\n",
       " ('industrial', '1ndustrlal'),\n",
       " ('consumers.', 'consumers.'),\n",
       " ('A', 'A'),\n",
       " ('study', 'study'),\n",
       " ('submitted', 'submltted'),\n",
       " ('by', 'by'),\n",
       " ('CMA', 'cm'),\n",
       " ('indicated', '1nd1cated'),\n",
       " ('that', 'that'),\n",
       " ('annual', 'annual'),\n",
       " ('cost', 'cost'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('industry', '1ndustry'),\n",
       " ('would', 'would'),\n",
       " ('be', 'be'),\n",
       " ('approximately', 'approxlmately'),\n",
       " ('$660', '$550'),\n",
       " ('million.', 'million.'),\n",
       " (\"FERC's\", \"FERC'S\"),\n",
       " ('action', 'actlon'),\n",
       " ('is', '15'),\n",
       " ('a', 'a'),\n",
       " ('major', 'major'),\n",
       " ('victory', 'Vlctory'),\n",
       " ('for', 'for'),\n",
       " ('CMA', 'cm'),\n",
       " ('and', 'and'),\n",
       " ('other', 'other'),\n",
       " ('industrial', '1ndustrlal'),\n",
       " ('users.', 'users.'),\n",
       " ('However,', 'However.'),\n",
       " ('the', 'the'),\n",
       " ('Commission', 'Commlsslon'),\n",
       " ('is', '15'),\n",
       " ('continuing', 'contlnulng'),\n",
       " ('its', '1::'),\n",
       " ('assessment', 'assessment'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('advantages', 'advantages'),\n",
       " ('and', 'and'),\n",
       " ('disadvantages', 'dlsadvantages'),\n",
       " ('of', 'of'),\n",
       " ('block', 'block'),\n",
       " ('billing', 'bllllng'),\n",
       " ('and', 'and'),\n",
       " ('further', 'further'),\n",
       " ('action', 'actlon'),\n",
       " ('is', '15'),\n",
       " ('still', '5:111'),\n",
       " ('possible.', 'posslble.'),\n",
       " ('II.', '11.'),\n",
       " ('ISSUES', 'ISSUES'),\n",
       " ('AND', 'AND'),\n",
       " ('PROGRAM', 'woman'),\n",
       " ('STATUS', '5mm:'),\n",
       " ('Hazardous', 'Hazardous'),\n",
       " ('Waste', 'Waste'),\n",
       " ('(Superfund)', '(Superfund)'),\n",
       " ('Congressional', 'Congresslonal'),\n",
       " ('Developments', 'Developments'),\n",
       " ('and', 'and'),\n",
       " ('Response', 'Response'),\n",
       " ('Senate', 'Senate'),\n",
       " ('Activity', 'Actlvlty'),\n",
       " ('The', 'The'),\n",
       " ('Senate', 'Senate'),\n",
       " ('passed', 'passed'),\n",
       " ('H.R.', '3.x.'),\n",
       " ('2005', '2005'),\n",
       " ('on', 'on'),\n",
       " ('September', 'September'),\n",
       " ('26,', '25,'),\n",
       " ('the', 'the'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('reauthorization', 'reauthorlzatlon'),\n",
       " ('for', 'for'),\n",
       " ('five', 'Elva'),\n",
       " ('years.', 'years.'),\n",
       " ('The', 'The'),\n",
       " ('legislation', 'leglslatlon'),\n",
       " ('will', 'Hill'),\n",
       " ('increase', '1ncrease'),\n",
       " ('nearly', 'nearly'),\n",
       " ('fivefold', 'flvefold'),\n",
       " ('the', 'the'),\n",
       " ('money', 'money'),\n",
       " ('available', 'avaliable'),\n",
       " ('to', 'to'),\n",
       " ('clean', 'clean'),\n",
       " ('up', 'up'),\n",
       " ('abandoned', 'abandoned'),\n",
       " ('hazardous', 'hazardous'),\n",
       " ('waste', 'waste'),\n",
       " ('sites', 'sltes'),\n",
       " ('by', 'by'),\n",
       " ('freezing', 'freezlng'),\n",
       " ('the', 'the'),\n",
       " ('feedstock', 'feedstock'),\n",
       " ('tax', 'tax'),\n",
       " ('and', 'and'),\n",
       " ('imposing', 'unposlng'),\n",
       " ('a', 'a'),\n",
       " ('broad-based', 'broadibased'),\n",
       " ('excise', 'exclse'),\n",
       " ('tax', 'tax'),\n",
       " ('on', 'on'),\n",
       " ('all', 'all'),\n",
       " ('manufacturers.', 'manufacturers.'),\n",
       " ('Senator', 'Senator'),\n",
       " ('Robert', 'Robert'),\n",
       " ('Packwood', 'Packwood'),\n",
       " ('(R-OR)', 'miom'),\n",
       " ('and', 'and'),\n",
       " ('other', 'other'),\n",
       " ('members', 'members'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('Finance', 'Flnance'),\n",
       " ('Committee', 'Commlttee'),\n",
       " ('put', 'put'),\n",
       " ('the', 'the'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('funding', 'fundlng'),\n",
       " ('package', 'package'),\n",
       " ('in', '1n'),\n",
       " ('the', 'the'),\n",
       " ('budget', 'budget'),\n",
       " ('reconciliation', 'reconclllatlon'),\n",
       " ('bill', 'mu'),\n",
       " ('which', 'whlch'),\n",
       " ('passed', 'passed'),\n",
       " ('the', 'the'),\n",
       " ('Senate.', 'Senate.'),\n",
       " ('Senator', 'Senator'),\n",
       " ('Jesse', 'Jesse'),\n",
       " ('Helms', 'Helms'),\n",
       " ('(R-NC)', 'âHam'),\n",
       " ('offered', 'offered'),\n",
       " ('an', 'an'),\n",
       " ('amendment', 'amendment'),\n",
       " ('to', 'to'),\n",
       " ('delete', 'delete'),\n",
       " ('the', 'the'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('funding', 'fundlng'),\n",
       " ('from', 'from'),\n",
       " ('the', 'the'),\n",
       " ('budget', 'budget'),\n",
       " ('reconciliation', 'reconclllatlon'),\n",
       " ('legislation,', 'leglslatlon.'),\n",
       " ('but', 'but'),\n",
       " ('he', 'he'),\n",
       " ('lost', 'lost'),\n",
       " ('66', 'as'),\n",
       " ('to', 'm'),\n",
       " ('32.', '32.'),\n",
       " ('The', 'The'),\n",
       " ('Senate,', 'SenatE,'),\n",
       " ('once', 'once'),\n",
       " ('again,', 'agaln,'),\n",
       " ('strongly', 'strongly'),\n",
       " ('confirmed', 'conflmed'),\n",
       " ('its', '1::'),\n",
       " ('support', 'support'),\n",
       " ('for', 'for'),\n",
       " ('a', 'a'),\n",
       " ('broad-based', 'broadibased'),\n",
       " ('tax', 'tax'),\n",
       " ('and', 'and'),\n",
       " ('freezing', 'freezlng'),\n",
       " ('feedstock', 'feedstock'),\n",
       " ('to', 'to'),\n",
       " ('finance', 'flnance'),\n",
       " ('the', 'the'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('cleanup', 'cleanup'),\n",
       " ('program.', 'program.'),\n",
       " ('House', 'House'),\n",
       " ('Activity', 'Actlvlty'),\n",
       " ('The', 'The'),\n",
       " ('House', 'House'),\n",
       " ('voted', 'Voted'),\n",
       " ('391-33', '391733'),\n",
       " ('on', 'on'),\n",
       " ('Dec.', 'Dec.'),\n",
       " ('10', '10'),\n",
       " ('to', 'm'),\n",
       " ('reauthorize', 'reauthorlze'),\n",
       " ('the', 'the'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('cleanup', 'cleanup'),\n",
       " ('program', 'program'),\n",
       " ('by', 'by'),\n",
       " ('significantly', 'slgnlflcantly'),\n",
       " ('increasing', '1ncreaslng'),\n",
       " ('the', 'the'),\n",
       " ('tax', 'tax'),\n",
       " ('of', 'of'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('feedstocks', 'feedstock:'),\n",
       " ('and', 'and'),\n",
       " ('oil.', '011.'),\n",
       " ('The', 'The'),\n",
       " ('House', 'House'),\n",
       " ('narrowly', 'narrowly'),\n",
       " ('defeated', 'defeated'),\n",
       " ('the', 'the'),\n",
       " ('Ways', 'Ways'),\n",
       " ('&', '&'),\n",
       " ('Means', 'Means'),\n",
       " (\"Committee's\", \"Commlttee's\"),\n",
       " ('broad-based', 'broadibased'),\n",
       " ('tax', 'tax'),\n",
       " ('which', 'whlch'),\n",
       " ('was', 'was'),\n",
       " ('opposed', 'opposed'),\n",
       " ('by', 'by'),\n",
       " ('President', 'Presldent'),\n",
       " ('Reagan,', 'Reagan,'),\n",
       " ('most', 'most'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('unions,', 'muons,'),\n",
       " ('the', 'the'),\n",
       " ('environmentalists,', 'envlronmentallsts,'),\n",
       " ('and', 'and'),\n",
       " ('a', 'a'),\n",
       " ('broad', 'broad'),\n",
       " ('coalition', 'Coalltlon'),\n",
       " ('of', 'of'),\n",
       " ('manufacturers,', 'manufacturers,'),\n",
       " ('with', 'Hlth'),\n",
       " ('this', 'thls'),\n",
       " ('Page', 'Page'),\n",
       " ('5', '5'),\n",
       " ('CMA', 'cm'),\n",
       " ('038609', '039509'),\n",
       " ('support,', 'support,'),\n",
       " ('the', 'the'),\n",
       " ('House', 'House'),\n",
       " ('passed', 'passed'),\n",
       " ('Representatives', 'Representatlves'),\n",
       " ('Thomas', 'Thomas'),\n",
       " (\"Downey's\", \"Downey's\"),\n",
       " ('(D-NY)', '(Drum'),\n",
       " ('and', 'and'),\n",
       " ('Bill', '13111'),\n",
       " (\"Frenzel's\", \"Frenzel's\"),\n",
       " ('(R-MN)', 'mimm'),\n",
       " ('amendment,', 'amendment.'),\n",
       " ('220-206,', '2207206,'),\n",
       " ('to', 'to'),\n",
       " ('finance', 'flnance'),\n",
       " ('the', 'the'),\n",
       " ('bulk', 'bulk'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('S10.3', '510.3'),\n",
       " ('billion', 'bllllon'),\n",
       " ('Superfund', 'Superfund'),\n",
       " ('from', 'from'),\n",
       " ('a', 'a'),\n",
       " ('$2', '$2'),\n",
       " ('billion', 'bllllon'),\n",
       " ('tax', 'tax'),\n",
       " ('on', 'on'),\n",
       " ('chemical', 'chemlcal'),\n",
       " ('feedstocks,', 'feedstocks,'),\n",
       " ('S3.1', '53.1'),\n",
       " ('billion', 'bllllon'),\n",
       " ('from', 'from'),\n",
       " ...]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Only consider edit distance = 1 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_typo.map(true_typo_dict)\n",
    "# cleaned_typo\n",
    "ed_1_typo = []\n",
    "from nltk import edit_distance\n",
    "for i in range(len(true_typo)):\n",
    "    typo = true_typo.loc[i,'typo']\n",
    "    correct = true_typo.loc[i,'correct']\n",
    "    if edit_distance(typo,correct) == 1:\n",
    "        ed_1_typo.append([typo,correct])\n",
    "                         \n",
    "ed_1_typo_df = pd.DataFrame(ed_1_typo)\n",
    "ed_1_typo_df.columns = ['typo','correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typo</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>communlcatlons</td>\n",
       "      <td>communications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>companles</td>\n",
       "      <td>companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>provlde</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thls</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nclud</td>\n",
       "      <td>including</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>heavlly</td>\n",
       "      <td>heavily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nvolved</td>\n",
       "      <td>involved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>crltlcal</td>\n",
       "      <td>critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>envlronmental</td>\n",
       "      <td>environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lssues</td>\n",
       "      <td>issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dent</td>\n",
       "      <td>identified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>natlonal</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>leglslators</td>\n",
       "      <td>legislators</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dlsposal</td>\n",
       "      <td>disposal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hill</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>contlnue</td>\n",
       "      <td>continue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>m</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>superfund</td>\n",
       "      <td>superfunds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>whlch</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>detalled</td>\n",
       "      <td>detailed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cm</td>\n",
       "      <td>cma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rlghtitoiknow</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>actlon</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>contlnues</td>\n",
       "      <td>continues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>toxlc</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>preventlon</td>\n",
       "      <td>prevention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>requlres</td>\n",
       "      <td>requires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>reportlng</td>\n",
       "      <td>reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nformatlon</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12081</th>\n",
       "      <td>costsharlng</td>\n",
       "      <td>costsharing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12082</th>\n",
       "      <td>attalned</td>\n",
       "      <td>attained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12083</th>\n",
       "      <td>rrwyy</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>rellablllty</td>\n",
       "      <td>reliability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12085</th>\n",
       "      <td>mm</td>\n",
       "      <td>ieee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12086</th>\n",
       "      <td>n</td>\n",
       "      <td>insulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12087</th>\n",
       "      <td>condult</td>\n",
       "      <td>conduit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12088</th>\n",
       "      <td>multklateral</td>\n",
       "      <td>multi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12089</th>\n",
       "      <td>misirm</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12090</th>\n",
       "      <td>dilarmly</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12091</th>\n",
       "      <td>fomally</td>\n",
       "      <td>formally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>stlpulated</td>\n",
       "      <td>stipulated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12093</th>\n",
       "      <td>busm</td>\n",
       "      <td>busin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12094</th>\n",
       "      <td>e</td>\n",
       "      <td>ess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12095</th>\n",
       "      <td>envuonmenm</td>\n",
       "      <td>environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12096</th>\n",
       "      <td>followiupactlons</td>\n",
       "      <td>follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12097</th>\n",
       "      <td>mm</td>\n",
       "      <td>unep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12098</th>\n",
       "      <td>anuqndusmy</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12099</th>\n",
       "      <td>blases</td>\n",
       "      <td>biases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12100</th>\n",
       "      <td>amvrcy</td>\n",
       "      <td>cefic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12101</th>\n",
       "      <td>chemlcalsprogramme</td>\n",
       "      <td>chemicalsprogramme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12102</th>\n",
       "      <td>mghaevel</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12103</th>\n",
       "      <td>meeungv</td>\n",
       "      <td>meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12104</th>\n",
       "      <td>lessor</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>complle</td>\n",
       "      <td>compile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>confldentlallnfomatlon</td>\n",
       "      <td>confidentialinformation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>cononunltylsmm</td>\n",
       "      <td>cononunity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>directlve</td>\n",
       "      <td>directive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12109</th>\n",
       "      <td>optlonsby</td>\n",
       "      <td>optionsby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12110</th>\n",
       "      <td>nev</td>\n",
       "      <td>inevitably</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12111 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         typo                  correct\n",
       "0              communlcatlons           communications\n",
       "1                   companles                companies\n",
       "2                     provlde                  provide\n",
       "3                        thls                     this\n",
       "4                       nclud                including\n",
       "5                     heavlly                  heavily\n",
       "6                     nvolved                 involved\n",
       "7                           n                       in\n",
       "8                    crltlcal                 critical\n",
       "9               envlronmental            environmental\n",
       "10                     lssues                   issues\n",
       "11                       dent               identified\n",
       "12                   natlonal                 national\n",
       "13                leglslators              legislators\n",
       "14                   dlsposal                 disposal\n",
       "15                       hill                     will\n",
       "16                   contlnue                 continue\n",
       "17                          m                       to\n",
       "18                  superfund               superfunds\n",
       "19                      whlch                    which\n",
       "20                   detalled                 detailed\n",
       "21                         cm                      cma\n",
       "22              rlghtitoiknow                    right\n",
       "23                     actlon                   action\n",
       "24                  contlnues                continues\n",
       "25                      toxlc                    toxic\n",
       "26                 preventlon               prevention\n",
       "27                   requlres                 requires\n",
       "28                  reportlng                reporting\n",
       "29                 nformatlon              information\n",
       "...                       ...                      ...\n",
       "12081             costsharlng              costsharing\n",
       "12082                attalned                 attained\n",
       "12083                   rrwyy                        r\n",
       "12084             rellablllty              reliability\n",
       "12085                      mm                     ieee\n",
       "12086                       n               insulation\n",
       "12087                 condult                  conduit\n",
       "12088            multklateral                    multi\n",
       "12089                  misirm                        d\n",
       "12090                dilarmly                        d\n",
       "12091                 fomally                 formally\n",
       "12092              stlpulated               stipulated\n",
       "12093                    busm                    busin\n",
       "12094                       e                      ess\n",
       "12095              envuonmenm              environment\n",
       "12096        followiupactlons                   follow\n",
       "12097                      mm                     unep\n",
       "12098              anuqndusmy                     anti\n",
       "12099                  blases                   biases\n",
       "12100                  amvrcy                    cefic\n",
       "12101      chemlcalsprogramme       chemicalsprogramme\n",
       "12102                mghaevel                     high\n",
       "12103                 meeungv                  meeting\n",
       "12104                  lessor                       or\n",
       "12105                 complle                  compile\n",
       "12106  confldentlallnfomatlon  confidentialinformation\n",
       "12107          cononunltylsmm               cononunity\n",
       "12108               directlve                directive\n",
       "12109               optlonsby                optionsby\n",
       "12110                     nev               inevitably\n",
       "\n",
       "[12111 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_typo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cba3598e4c140c3843a5cc03571aa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5901), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "typos = ed_1_typo_df['typo']\n",
    "correct = ed_1_typo_df['correct']\n",
    "\n",
    "Correction_output,no_correction_num,no_correct_word = Correction(typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrected rate: 4.5%\n"
     ]
    }
   ],
   "source": [
    "print('No corrected rate: {:.1%}'.format(no_correction_num/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.44%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(vintersection(Correction_output,correct)/len(Correction_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correction_output.to_csv('../output/Correction_output_ed_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Recall & precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3995770014099953"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(Correction_output,correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12075087586402802"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(Correction_output,correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5901"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                companies\n",
       "1                  provide\n",
       "2                     this\n",
       "3                  heavily\n",
       "4                 involved\n",
       "5                        n\n",
       "6            environmental\n",
       "7                   issues\n",
       "8                 national\n",
       "9              legislators\n",
       "10                disposal\n",
       "11                    bill\n",
       "12                continue\n",
       "13               suparfund\n",
       "14                   which\n",
       "15                detailed\n",
       "16                     cma\n",
       "17                  action\n",
       "18               continues\n",
       "19                   toxic\n",
       "20              prevention\n",
       "21                requires\n",
       "22               reporting\n",
       "23              concerning\n",
       "24                industry\n",
       "25              operations\n",
       "26                    risk\n",
       "27               reduction\n",
       "28               cessation\n",
       "29                   issue\n",
       "               ...        \n",
       "5871            background\n",
       "5872                     u\n",
       "5873                    no\n",
       "5874                result\n",
       "5875               patents\n",
       "5876              together\n",
       "5877             taiwanese\n",
       "5878                canada\n",
       "5879                solely\n",
       "5880                dupont\n",
       "5881                     m\n",
       "5882              refusing\n",
       "5883          conventional\n",
       "5884                buying\n",
       "5885                  ldcs\n",
       "5886              marginal\n",
       "5887          depreciation\n",
       "5888                compar\n",
       "5889                 tally\n",
       "5890            unsettling\n",
       "5891           costsharing\n",
       "5892              attained\n",
       "5893               consult\n",
       "5894              formally\n",
       "5895            stipulated\n",
       "5896                 bases\n",
       "5897    chemicalsprogramme\n",
       "5898                comple\n",
       "5899             directive\n",
       "5900             optionsby\n",
       "Length: 5901, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
